[toc]

# [语音转录为文本](https://developer.apple.com/tutorials/app-dev-training/transcribing-speech-to-text)

在本教程中，您将向 Scrumdinger 添加一个功能，用于捕获和记录会议记录。 您将请求访问麦克风等设备硬件并集成语音框架以将实时音频转录为文本。

[下载](https://docs-assets.developer.apple.com/published/1ea2eec121b90031e354288912a76357/TranscribingSpeechToText.zip)启动项目并按照本教程进行操作，或者打开完成的项目并自行探索代码。

## 1. 请求对设备硬件的授权

Scrumdinger 从需要访问设备麦克风的录音中生成会议记录。 作为一项安全功能，用户必须明确授予对个人信息或敏感设备硬件的访问权限。 有关保护用户数据的更多信息，请参阅保[护用户隐私](https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy)。

在本节中，您将添加要求用户授予或拒绝访问权限的使用说明。

1. 添加设备请求

   * 在 info.plist 中，添加 Privacy - Speech Recognition Usage Description ，并添加描述 语音识别
   * 在 info.plist 中，添加 Privacy - Microphone Usage Description  ，并添加描述 麦克风

2. 添加 SpeechRecognizer.swift 文件

   ```swift
   // SpeechRecognizer.swift
   import AVFoundation
   import Foundation
   import Speech
   import SwiftUI
   
   class SpeechRecognizer: ObservableObject {
       enum RecognizerError: Error {
           case nilRecognizer
           case notAuthorizedToRecognize
           case notPermittedToRecord
           case recognizerIsUnavailable
           
           var message: String {
               switch self {
               case .nilRecognizer: return "Can't initialize speech recognizer"
               case .notAuthorizedToRecognize: return "Not authorized to recognize speech"
               case .notPermittedToRecord: return "Not permitted to record audio"
               case .recognizerIsUnavailable: return "Recognizer is unavailable"
               }
           }
       }
       
       var transcript: String = ""
       
       private var audioEngine: AVAudioEngine?
       private var request: SFSpeechAudioBufferRecognitionRequest?
       private var task: SFSpeechRecognitionTask?
       private let recognizer: SFSpeechRecognizer?
       
       init() {
           recognizer = SFSpeechRecognizer()
           
           Task(priority: .background) {
               do {
                   guard recognizer != nil else {
                       throw RecognizerError.nilRecognizer
                   }
                   guard await SFSpeechRecognizer.hasAuthorizationToRecognize() else {
                       throw RecognizerError.notAuthorizedToRecognize
                   }
                   guard await AVAudioSession.sharedInstance().hasPermissionToRecord() else {
                       throw RecognizerError.notPermittedToRecord
                   }
               } catch {
                   speakError(error)
               }
           }
       }
       
       deinit {
           reset()
       }
       
       func transcribe() {
           DispatchQueue(label: "Speech Recognizer Queue", qos: .background).async { [weak self] in
               guard let self = self, let recognizer = self.recognizer, recognizer.isAvailable else {
                   self?.speakError(RecognizerError.recognizerIsUnavailable)
                   return
               }
               
               do {
                   let (audioEngine, request) = try Self.prepareEngine()
                   self.audioEngine = audioEngine
                   self.request = request
                   self.task = recognizer.recognitionTask(with: request, resultHandler: self.recognitionHandler(result:error:))
               } catch {
                   self.reset()
                   self.speakError(error)
               }
           }
       }
       
       func stopTranscribing() {
           reset()
       }
       
       func reset() {
           task?.cancel()
           audioEngine?.stop()
           audioEngine = nil
           request = nil
           task = nil
       }
       
       private static func prepareEngine() throws -> (AVAudioEngine, SFSpeechAudioBufferRecognitionRequest) {
           let audioEngine = AVAudioEngine()
           
           let request = SFSpeechAudioBufferRecognitionRequest()
           request.shouldReportPartialResults = true
           
           let audioSession = AVAudioSession.sharedInstance()
           try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
           try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
           let inputNode = audioEngine.inputNode
           
           let recordingFormat = inputNode.outputFormat(forBus: 0)
           inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer: AVAudioPCMBuffer, when: AVAudioTime) in
               request.append(buffer)
           }
           audioEngine.prepare()
           try audioEngine.start()
           
           return (audioEngine, request)
       }
       
       private func recognitionHandler(result: SFSpeechRecognitionResult?, error: Error?) {
           let receivedFinalResult = result?.isFinal ?? false
           let receivedError = error != nil
           
           if receivedFinalResult || receivedError {
               audioEngine?.stop()
               audioEngine?.inputNode.removeTap(onBus: 0)
           }
           
           if let result = result {
               speak(result.bestTranscription.formattedString)
           }
       }
       
       private func speak(_ message: String) {
           transcript = message
       }
       
       private func speakError(_ error: Error) {
           var errorMessage = ""
           if let error = error as? RecognizerError {
               errorMessage += error.message
           } else {
               errorMessage += error.localizedDescription
           }
           transcript = "<< \(errorMessage) >>"
       }
   }
   
   extension SFSpeechRecognizer {
       static func hasAuthorizationToRecognize() async -> Bool {
           await withCheckedContinuation { continuation in
               requestAuthorization { status in
                   continuation.resume(returning: status == .authorized)
               }
           }
       }
   }
   
   extension AVAudioSession {
       func hasPermissionToRecord() async -> Bool {
           await withCheckedContinuation { continuation in
               requestRecordPermission { authorized in
                   continuation.resume(returning: authorized)
               }
           }
       }
   }
   ```

   

## 2. 集成语音识别

在本节中，您将通过在视图生命周期的特定点调用方法，将语音识别器集成到会议视图中。

1. 在 MeetingView 中添加 @StateObject 属性  speechRecognizer,

   ```swift
   // MeetingView.swift
   + @StateObject var speechRecognizer = SpeechRecognizer()
   + @State private var isRecording = false
   ```

2. 在 onAppear 和 onDisappear 中添加方法

   ```swift
   // MeetingView.swift
   .onAppear {
       scrumTimer.reset(lengthInMinutes: scrum.lengthInMinutes, attendees: scrum.attendees)
       scrumTimer.speakerChangedAction = {
           player.seek(to: .zero)
           player.play()
       }
   +    speechRecognizer.reset()
   +    speechRecognizer.transcribe()
   +    isRecording = true
       scrumTimer.startScrum()
   }
   .onDisappear {
       scrumTimer.stopScrum()
   +    speechRecognizer.stopTranscribing()
   +    isRecording = false
       let newHistory = History(attendees: scrum.attendees, lengthInMinutes: scrum.timer.secondsElapsed / 60)
       scrum.history.insert(newHistory, at: 0)
   }
   ```

## 3. 显示记录指示器

现在您的应用程序可以转录会议，您将添加视觉元素和辅助功能，以便在转录正在进行时通知用户。

1. MeetingTimerView 增加 isRecording 属性，并对 MeetingView 中初始化时传值

   ```swift
   // MeetingTimerView.swift
   let isRecording: Bool
   
   // MeetingView.swift
   - MeetingTimerView(speakers: scrumTimer.speakers, theme: scrum.theme)
   + MeetingTimerView(speakers: scrumTimer.speakers, isRecording: isRecording, theme: scrum.theme)
   ```

2. 根据 isRecording，显示不同语音图片

   ```swift
   // MeetingTimerView.swift
   VStack {
       Text(currentSpeaker)
           .font(.title)
       Text("is speaking")
   +  	Image(systemName: isRecording ? "mic" : "mic.slash")
   +        .font(.title)
   +        .padding(.top)
   +        .accessibilityLabel(isRecording ? "with transcription" : "without transcription")
   }
   ```

   

## 4. 创建历史视图

接下来，您将构建 Scrumdinger 的最后一个视图。 您已使用语音识别器转录会议。 现在，您将创建一个显示会议文本和与会者的历史记录视图。

1. 在 History 模型中，增加 transcript 的可选属性，并在初始化方法中添加对应 nil 参数

   ```swift
   // History.swift
   struct History: Identifiable, Codable {
     // ... 
     var transcript: Sting?
     
     init( .... , transcript: String? = nil) {
       // ...
       self.transcript = transcript
     }
   }
   ```

2. 在 MeetingView 的 onDisappear 方法中，生成 History 对象时，增加 transcript 属性

   ```swift
   /// MeetingView.swift
   .onDisappear {
     // ...
     let newHistory = History(attendees: scrum.attendees, lengthInMinutes: scrum.timer.secondsElapsed / 60, transcript: speechRecognizer.transcript)
     // ...
   }
   ```

3. 创建 History View ，并展示

   ```swift
   // HistroyView.swift
   struct HistoryView: View {
       let history: History
   
       var body: some View {
           ScrollView {
               VStack(alignment: .leading) {
                   Divider()
                       .padding(.bottom)
                   Text("Attendees")
                       .font(.headline)
                   Text(history.attendeeString)
                   if let transcript = history.transcript {
                       Text("Transcript")
                           .font(.headline)
                           .padding(.top)
                       Text(transcript)
                   }
               }
           }
           .navigationTitle(Text(history.date, style: .date))
           .padding()
       }
   }
   
   extension History {
       var attendeeString: String {
           ListFormatter.localizedString(byJoining: attendees.map { $0.name })
       }
   }
   
   struct HistoryView_Previews: PreviewProvider {
       static var history: History {
           History(attendees: [DailyScrum.Attendee(name: "Jon"), DailyScrum.Attendee(name: "Darla"), DailyScrum.Attendee(name: "Luis")], lengthInMinutes: 10, transcript: "Darla, would you like to start today? Sure, yesterday I reviewed Luis' PR and met with the design team to finalize the UI...")
       }
       
       static var previews: some View {
           HistoryView(history: history)
       }
   }
   ```

## 5. 展示历史视图

最后，您将把历史视图集成到 Scrumdinger 中。 您将测试该应用程序以确保您可以查看每个 Scrum 的历史记录。

```swift
// DetailView.swift
Section(header: Text("History")) {
    if scrum.history.isEmpty {
        Label("No meetings yet", systemImage: "calendar.badge.exclamationmark")
    }
    ForEach(scrum.history) { history in
        NavigationLink(destination: HistoryView(history: history)) {
            HStack {
                Image(systemName: "calendar")
                Text(history.date, style: .date)
            }
+        }
    }
}
```

